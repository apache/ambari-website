"use strict";(self.webpackChunkambari_website=self.webpackChunkambari_website||[]).push([[8565],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=p(a),m=r,d=c["".concat(l,".").concat(m)]||c[m]||h[m]||s;return a?n.createElement(d,o(o({ref:t},u),{},{components:a})):n.createElement(d,o({ref:t},u))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=a.length,o=new Array(s);o[0]=c;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,o[1]=i;for(var p=2;p<s;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},7417:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const s={},o="Blueprint Support for HA Clusters",i={unversionedId:"ambari-design/blueprints/blueprint-ha",id:"version-2.7.6/ambari-design/blueprints/blueprint-ha",title:"Blueprint Support for HA Clusters",description:"Summary",source:"@site/versioned_docs/version-2.7.6/ambari-design/blueprints/blueprint-ha.md",sourceDirName:"ambari-design/blueprints",slug:"/ambari-design/blueprints/blueprint-ha",permalink:"/docs/ambari-design/blueprints/blueprint-ha",draft:!1,editUrl:"https://github.com/vivostar/vivostar.github.io/tree/master/versioned_docs/version-2.7.6/ambari-design/blueprints/blueprint-ha.md",tags:[],version:"2.7.6",frontMatter:{},sidebar:"ambariSidebar",previous:{title:"Blueprints",permalink:"/docs/blueprints"},next:{title:"Blueprint support for Ranger",permalink:"/docs/ambari-design/blueprints/blueprint-ranger"}},l={},p=[{value:"Summary",id:"summary",level:2},{value:"FAQ",id:"faq",level:3},{value:"Compatibility with Ambari UI\xa0",id:"compatibility-with-ambari-ui",level:4},{value:"Supported Stack Versions",id:"supported-stack-versions",level:4},{value:"Examples",id:"examples",level:3},{value:"Blueprint Example: HDFS NameNode HA Cluster",id:"blueprint-example-hdfs-namenode-ha-cluster",level:4},{value:"How",id:"how",level:4},{value:"Configuring Active and Standby NameNodes",id:"configuring-active-and-standby-namenodes",level:4},{value:"Example Blueprint",id:"example-blueprint",level:4},{value:"HostName Topology Substitution in Configuration Property Values",id:"hostname-topology-substitution-in-configuration-property-values",level:4},{value:"Register Blueprint with Ambari Server \xa0",id:"register-blueprint-with-ambari-server-",level:4},{value:"Example Cluster Creation Template",id:"example-cluster-creation-template",level:4},{value:"Create Cluster Instance",id:"create-cluster-instance",level:4},{value:"Blueprint Example: Yarn ResourceManager HA Cluster",id:"blueprint-example-yarn-resourcemanager-ha-cluster",level:3},{value:"Summary",id:"summary-1",level:4},{value:"Initial setup of Active and Standby ResourceManagers",id:"initial-setup-of-active-and-standby-resourcemanagers",level:4},{value:"Example Blueprint",id:"example-blueprint-1",level:4},{value:"Register Blueprint with Ambari Server",id:"register-blueprint-with-ambari-server",level:4},{value:"Example Cluster Creation Template",id:"example-cluster-creation-template-1",level:4},{value:"Create Cluster Instance",id:"create-cluster-instance-1",level:4},{value:"Blueprint Example: HBase RegionServer HA Cluster",id:"blueprint-example-hbase-regionserver-ha-cluster",level:3},{value:"Summary",id:"summary-2",level:4},{value:"Example Blueprint",id:"example-blueprint-2",level:4},{value:"Register Blueprint with Ambari Server",id:"register-blueprint-with-ambari-server-1",level:4},{value:"Example Cluster Creation Template",id:"example-cluster-creation-template-2",level:4},{value:"Create Cluster Instance",id:"create-cluster-instance-2",level:4}],u={toc:p};function h(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"blueprint-support-for-ha-clusters"},"Blueprint Support for HA Clusters"),(0,r.kt)("h2",{id:"summary"},"Summary"),(0,r.kt)("p",null,"As of Ambari 2.0, Blueprints are able to deploy the following components with HA:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"\xa0HDFS NameNode HA")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"YARN ResourceManager HA")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"HBase RegionServers HA"))),(0,r.kt)("p",null,"As of Ambari 2.1, Blueprints are able to deploy the following components with HA:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Hive Components (",(0,r.kt)("a",{parentName:"li",href:"https://issues.apache.org/jira/browse/AMBARI-10489"},"AMBARI-10489"),")"),(0,r.kt)("li",{parentName:"ul"},"Storm Nimbus (",(0,r.kt)("a",{parentName:"li",href:"https://issues.apache.org/jira/browse/AMBARI-11087"},"AMBARI-11087"),")"),(0,r.kt)("li",{parentName:"ul"},"Oozie Server (",(0,r.kt)("a",{parentName:"li",href:"https://issues.apache.org/jira/browse/AMBARI-6683"},"AMBARI-6683"),")  ")),(0,r.kt)("p",null,"This functionality currently requires providing fine-grained configurations. This document provides examples."),(0,r.kt)("h3",{id:"faq"},"FAQ"),(0,r.kt)("h4",{id:"compatibility-with-ambari-ui"},"Compatibility with Ambari UI\xa0"),(0,r.kt)("p",null,"While this feature does not require the Ambari UI to function, the Blueprints HA feature is completely compatible with the Ambari UI. \xa0An HA cluster created via Blueprints can be monitored and configured via the Ambari UI, just as any other Blueprints cluster would function. \xa0\xa0\xa0"),(0,r.kt)("h4",{id:"supported-stack-versions"},"Supported Stack Versions"),(0,r.kt)("p",null,"This feature is supported as of HDP 2.1 and newer releases. Previous versions of HDP have not been tested with this feature. \xa0"),(0,r.kt)("h3",{id:"examples"},"Examples"),(0,r.kt)("h4",{id:"blueprint-example-hdfs-namenode-ha-cluster"},"Blueprint Example: HDFS NameNode HA Cluster"),(0,r.kt)("p",null,"HDFS NameNode HA allows a cluster to be configured such that a NameNode is not a single point of failure."),(0,r.kt)("p",null,"For more details on\xa0",(0,r.kt)("a",{parentName:"p",href:"http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html"},"HDFS NameNode HA see the Apache Hadoop documentation"),"."),(0,r.kt)("p",null,"In an Ambari-deployed HDFS NameNode HA cluster:  "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"2 NameNodes are deployed: an \u201cactive\u201d and a \u201cpassive\u201d namenode."),(0,r.kt)("li",{parentName:"ul"},"If the active NameNode should stop functioning properly, the passive node\u2019s Zookeeper client will detect this case, and the passive node will become the new active node."),(0,r.kt)("li",{parentName:"ul"},"HDFS relies on Zookeeper to manage the details of failover in these cases."),(0,r.kt)("li",{parentName:"ul"},"The Blueprints HA feature will automatically invoke all required commands and setup steps for an HDFS NameNode HA cluster, provided that the correct configuration is provided in the Blueprint. \xa0The shared edit logs of each NameNode are managed by the Quorum Journal Manager, rather than NFS shared storage. \xa0The use of NFS shared storage in an HDFS HA setup is not supported by Ambari Blueprints, and is generally not encouraged. \xa0")),(0,r.kt)("h4",{id:"how"},"How"),(0,r.kt)("p",null,"The Blueprints HA feature will automatically invoke all required commands and setup steps for an HDFS NameNode HA cluster, provided that the correct configuration is provided in the Blueprint. \xa0The shared edit logs of each NameNode are managed by the Quorum Journal Manager, rather than NFS shared storage. \xa0The use of NFS shared storage in an HDFS HA setup is not supported by Ambari Blueprints, and is generally not encouraged. \xa0"),(0,r.kt)("p",null,"The following HDFS stack components should be included in any host group in a Blueprint that supports an HA HDFS NameNode:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"NAMENODE"),(0,r.kt)("li",{parentName:"ol"},"ZKFC"),(0,r.kt)("li",{parentName:"ol"},"ZOOKEEPER_SERVER"),(0,r.kt)("li",{parentName:"ol"},"JOURNALNODE")),(0,r.kt)("h4",{id:"configuring-active-and-standby-namenodes"},"Configuring Active and Standby NameNodes"),(0,r.kt)("p",null,"The HDFS \u201cNAMENODE\u201d component must be assigned to two servers, either via two separate host groups, or to a host group that maps to two physical servers in the Cluster Creation Template for this cluster. \xa0"),(0,r.kt)("p",null,"By default, the Blueprint processor will assign the \u201cactive\u201d NameNode to one host, and the \u201cstandby\u201d NameNode to another. \xa0The user of an HA Blueprint does not need to configure the initial status of each NameNode, since this can be assigned automatically. \xa0"),(0,r.kt)("p",null,"If desired, the user can configure the initial state of each NameNode by adding the following configuration properties in the \u201chadoop-env\u201d namespace:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"dfs_ha_initial_namenode_active - This property should contain the hostname for the \u201cactive\u201d NameNode in this cluster."),(0,r.kt)("li",{parentName:"ol"},"dfs_ha_initial_namenode_standby - This property should contain the host name for the \u201cpassive\u201d NameNode in this cluster.")),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"These properties should only be used when the initial state of the active or standby NameNodes needs to be configured to a specific node. \xa0This setting is only guaranteed to be accurate in the initial state of the cluster. \xa0Over time, the active/standby state of each NameNode may change as failover events occur in the cluster."),(0,r.kt)("p",{parentName:"admonition"},"The active or standby status of a NameNode is not recorded or expressed when an HDFS HA Cluster is being exported to a Blueprint, using the Blueprint REST API endpoint. \xa0Since clusters change over time, this state is only accurate in the initial startup of the cluster."),(0,r.kt)("p",{parentName:"admonition"},"Generally, it is assumed that most users will not need to choose the active or standby status of each NameNode, so the default behavior in Blueprints HA is to assign the status of each node automatically.")),(0,r.kt)("h4",{id:"example-blueprint"},"Example Blueprint"),(0,r.kt)("p",null,"This is a minimal blueprint with HDFS HA:\xa0",(0,r.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/download/attachments/55151584/hdfs_ha_blueprint.json?version=4&modificationDate=1434548806000&api=v2"},"hdfs_ha_blueprint.json"),"\xa0"),(0,r.kt)("p",null,"These are the base configurations required. See the blueprint above for more details:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'  "configurations": \n    { "core-site": {\n        "properties" : {\n          "fs.defaultFS" : "hdfs://mycluster",\n          "ha.zookeeper.quorum" : "%HOSTGROUP::master_1%:2181,%HOSTGROUP::master_2%:2181,%HOSTGROUP::master_3%:2181"\n      }}\n    },\n    { "hdfs-site": {\n        "properties" : {\n          "dfs.client.failover.proxy.provider.mycluster" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",\n          "dfs.ha.automatic-failover.enabled" : "true",\n          "dfs.ha.fencing.methods" : "shell(/bin/true)",\n          "dfs.ha.namenodes.mycluster" : "nn1,nn2",\n          "dfs.namenode.http-address" : "%HOSTGROUP::master_1%:50070",\n          "dfs.namenode.http-address.mycluster.nn1" : "%HOSTGROUP::master_1%:50070",\n          "dfs.namenode.http-address.mycluster.nn2" : "%HOSTGROUP::master_3%:50070",\n          "dfs.namenode.https-address" : "%HOSTGROUP::master_1%:50470",\n          "dfs.namenode.https-address.mycluster.nn1" : "%HOSTGROUP::master_1%:50470",\n          "dfs.namenode.https-address.mycluster.nn2" : "%HOSTGROUP::master_3%:50470",\n          "dfs.namenode.rpc-address.mycluster.nn1" : "%HOSTGROUP::master_1%:8020",\n          "dfs.namenode.rpc-address.mycluster.nn2" : "%HOSTGROUP::master_3%:8020",\n          "dfs.namenode.shared.edits.dir" : "qjournal://%HOSTGROUP::master_1%:8485;%HOSTGROUP::master_2%:8485;%HOSTGROUP::master_3%:8485/mycluster",\n          "dfs.nameservices" : "mycluster"\n      }}\n    }\n  ]\n')),(0,r.kt)("h4",{id:"hostname-topology-substitution-in-configuration-property-values"},"HostName Topology Substitution in Configuration Property Values"),(0,r.kt)("p",null,"The host-related properties should be set using the \u201cHOSTGROUP\u201d syntax to refer to a given Blueprint\u2019s host group, in order to map each NameNode\u2019s actual host (defined in the Cluster Creation Template) to the properties in hdfs-site that require these host mappings. \xa0"),(0,r.kt)("p",null,"The syntax for these properties should be:"),(0,r.kt)("p",null,"\u201c%HOSTGROUP::HOST_GROUP_NAME%:PORT\u201d\u201d"),(0,r.kt)("p",null,"For example, the following property from the snippet above:"),(0,r.kt)("p",null,'"dfs.namenode.http-address.mycluster.nn1":'),(0,r.kt)("p",null,'"%HOSTGROUP::master_1%:50070"'),(0,r.kt)("p",null,"This property value is interpreted by the Blueprint processor to refer to the host that maps to the \u201cmaster_1\u201d host group, which should include a \u201cNAMENODE\u201d among its list of components. \xa0The address property listed above should map to the host for \u201cmaster_1\u201d, at the port \u201c50070\u201d. \xa0"),(0,r.kt)("p",null,"Using this syntax is the most portable way to define host-specific properties within a Blueprint, since no direct host name references are used. \xa0This will allow a Blueprint to be applied in a wider variety of cluster deployments, and not be tied to a specific set of hostnames. \xa0"),(0,r.kt)("h4",{id:"register-blueprint-with-ambari-server-"},"Register Blueprint with Ambari Server \xa0"),(0,r.kt)("p",null,'Post the blueprint to the "blueprint-hdfs-ha" resource to the Ambari Server.'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"POST /api/v1/blueprints/blueprint-hdfs-ha\n \n...\n[ Request Body is the example blueprint defined above ]\n...\n \n201 - Created\n")),(0,r.kt)("h4",{id:"example-cluster-creation-template"},"Example Cluster Creation Template"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "blueprint": "blueprint-hdfs-ha",\n  "default_password": "changethis",\n  "host_groups": [\n    { "hosts": [\n        { "fqdn": "c6401.ambari.apache.org" }\n      ], "name": "gateway"\n    },\n    { "hosts": [\n        { "fqdn": "c6402.ambari.apache.org" }\n      ], "name": "master_1"\n    },\n    { "hosts": [\n        { "fqdn": "c6403.ambari.apache.org" }\n      ], "name": "master_2"\n    },\n    { "hosts": [\n        { "fqdn": "c6404.ambari.apache.org" }\n      ], "name": "master_3"\n    },\n    { "hosts": [\n        { "fqdn": "c6405.ambari.apache.org" }\n      ],\n      "name": "slave_1"     \n    }\n  ]\n}\n')),(0,r.kt)("h4",{id:"create-cluster-instance"},"Create Cluster Instance"),(0,r.kt)("p",null,"Post the cluster to the Ambari Server to provision the cluster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'POST /api/v1/clusters/my-hdfs-ha-cluster\n \n...\n[ Request Body is above Cluster Creation Template ]\n...\n \n202 - Accepted\n{\n  "href" : "http://c6401.ambari.apache.org:8080/api/v1/clusters/my-hdfs-ha-cluster/requests/1",\n  "Requests" : {\n    "id" : 1,\n    "status" : "InProgress"\n  }\n}\n\xa0\n...\n[ Client can then monitor the URL in the 202 response to check the status of the cluster deployment. ]\n...\n')),(0,r.kt)("h3",{id:"blueprint-example-yarn-resourcemanager-ha-cluster"},"Blueprint Example: Yarn ResourceManager HA Cluster"),(0,r.kt)("h4",{id:"summary-1"},"Summary"),(0,r.kt)("p",null,"Yarn ResourceManager High Availability (HA) adds support for deploying two Yarn ResourceManagers in a given Yarn cluster. \xa0This support removes the single point of failure that occurs when single ResourceManager is used. \xa0"),(0,r.kt)("p",null,"The Yarn ResourceManager support for HA is somewhat similar to HDFS NameNode HA in that an \u201cactive/standby\u201d architecture is adopted, with Zookeeper used to handle the failover scenarios between the two ResourceManager instances. \xa0"),(0,r.kt)("p",null,"The following Apache Hadoop documentation describes the steps required in order to setup Yarn ResourceManager HA manually:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html"},"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html")),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"Ambari Blueprints will handle much of the details of server setup listed in this documentation, but the user of Ambari will need to define the various configuration properties listed in this article (yarn.resourcemanager.ha.enabled, yarn.resourcemanager.hostname.$NAME_OF_RESOURCE_MANAGER, etc). \xa0The example Blueprints listed below will demonstrate the configuration properties that must be included in the Blueprint for this feature to startup the HA cluster properly.")),(0,r.kt)("p",null,"\xa0\xa0The following stack components should be included in any host group in a Blueprint that supports an HA Yarn ResourceManager"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"RESOURCEMANAGER"),(0,r.kt)("li",{parentName:"ol"},"ZOOKEEPER_SERVER")),(0,r.kt)("h4",{id:"initial-setup-of-active-and-standby-resourcemanagers"},"Initial setup of Active and Standby ResourceManagers"),(0,r.kt)("p",null,"The Yarn ResourceManager HA feature depends upon Zookeeper in order to manage the details of \xa0the \u201cactive\u201d or \u201cstandby\u201d status of a given ResourceManager. \xa0When the two ResourceManagers are starting up initially, the first ResourceManager instance to acquire a Zookeeper lock, called a \u201cznode\u201d, will become the \u201cactive\u201d ResourceManager for the cluster, with the other instance assuming the role of the \u201cstandby\u201d ResourceManager. \xa0"),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"The Blueprints HA feature does not support configuring the initial \u201cactive\u201d or \u201cstandby\u201d status of the ResourceManagers deployed in a Yarn HA cluster. \xa0The first instance to obtain the Zookeeper lock will become the \u201cactive\u201d node. \xa0This allows the user to specify the host groups that contain the 2 ResourceManager instances, but also shields the user from the need to select the first \u201cactive\u201d node."),(0,r.kt)("p",{parentName:"admonition"},"After the cluster has started up initially, the state of the \u201cactive\u201d and \u201cstandby\u201d ResourceManagers may change over time. \xa0The initial \u201cactive\u201d server is not guaranteed to remain the \u201cactive\u201d node over the lifetime of the cluster. \xa0During a failover event, the \u201cstandby\u201d node may be required to fulfill the role of the \u201cactive\u201d server."),(0,r.kt)("p",{parentName:"admonition"},"The active or standby status of a ResourceManager is not recorded or expressed when a Yarn cluster is being exported to a Blueprint, using the Blueprint REST API endpoint. \xa0Since clusters change over time, this state is only accurate in the initial startup of the cluster.")),(0,r.kt)("h4",{id:"example-blueprint-1"},"Example Blueprint"),(0,r.kt)("p",null,"The following link includes an example Blueprint for a 3-node Yarn ResourceManager HA Cluster:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/download/attachments/55151584/yarn_ha_blueprint.json?version=2&modificationDate=1432208770000&api=v2"},"yarn_ha_blueprint.json"),"  "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "Blueprints": {\n    "stack_name": "HDP",\n    "stack_version": "2.2"\n  },\n  "host_groups": [\n    {\n      "name": "gateway",\n      "cardinality" : "1",\n      "components": [\n        { "name": "HDFS_CLIENT" },\n        { "name": "MAPREDUCE2_CLIENT" },\n        { "name": "METRICS_COLLECTOR" },\n        { "name": "METRICS_MONITOR" },\n        { "name": "TEZ_CLIENT" },\n        { "name": "YARN_CLIENT" },\n        { "name": "ZOOKEEPER_CLIENT" }\n      ]\n    },\n    {\n      "name": "master_1",\n      "cardinality" : "1",\n      "components": [\n        { "name": "HISTORYSERVER" },\n        { "name": "JOURNALNODE" },\n        { "name": "METRICS_MONITOR" },\n        { "name": "NAMENODE" },\n        { "name": "ZOOKEEPER_SERVER" }\n      ]\n    },\n    {\n      "name": "master_2",\n      "cardinality" : "1",\n      "components": [\n        { "name": "APP_TIMELINE_SERVER" },\n        { "name": "JOURNALNODE" },\n        { "name": "METRICS_MONITOR" },\n        { "name": "RESOURCEMANAGER" },\n        { "name": "ZOOKEEPER_SERVER" }\n      ]\n    },\n    {\n      "name": "master_3",\n      "cardinality" : "1",\n      "components": [\n        { "name": "JOURNALNODE" },\n        { "name": "METRICS_MONITOR" },\n        { "name": "RESOURCEMANAGER" },\n        { "name": "SECONDARY_NAMENODE" },\n        { "name": "ZOOKEEPER_SERVER" }\n      ]\n    },\n    {\n      "name": "slave_1",\n      "components": [\n        { "name": "DATANODE" },\n        { "name": "METRICS_MONITOR" },\n        { "name": "NODEMANAGER" }\n      ]\n    }\n  ],\n  "configurations": [\n    {\n      "core-site": {\n        "properties" : {\n          "fs.defaultFS" : "hdfs://%HOSTGROUP::master_1%:8020"\n      }}\n    },{\n      "yarn-site" : {\n        "properties" : {\n          "hadoop.registry.rm.enabled" : "false",\n          "hadoop.registry.zk.quorum" : "%HOSTGROUP::master_3%:2181,%HOSTGROUP::master_2%:2181,%HOSTGROUP::master_1%:2181",\n          "yarn.log.server.url" : "http://%HOSTGROUP::master_2%:19888/jobhistory/logs",\n          "yarn.resourcemanager.address" : "%HOSTGROUP::master_2%:8050",\n          "yarn.resourcemanager.admin.address" : "%HOSTGROUP::master_2%:8141",\n          "yarn.resourcemanager.cluster-id" : "yarn-cluster",\n          "yarn.resourcemanager.ha.automatic-failover.zk-base-path" : "/yarn-leader-election",\n          "yarn.resourcemanager.ha.enabled" : "true",\n          "yarn.resourcemanager.ha.rm-ids" : "rm1,rm2",\n          "yarn.resourcemanager.hostname" : "%HOSTGROUP::master_2%",\n          "yarn.resourcemanager.recovery.enabled" : "true",\n          "yarn.resourcemanager.resource-tracker.address" : "%HOSTGROUP::master_2%:8025",\n          "yarn.resourcemanager.scheduler.address" : "%HOSTGROUP::master_2%:8030",\n          "yarn.resourcemanager.store.class" : "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",\n          "yarn.resourcemanager.webapp.address" : "%HOSTGROUP::master_2%:8088",\n          "yarn.resourcemanager.webapp.https.address" : "%HOSTGROUP::master_2%:8090",\n          "yarn.timeline-service.address" : "%HOSTGROUP::master_2%:10200",\n          "yarn.timeline-service.webapp.address" : "%HOSTGROUP::master_2%:8188",\n          "yarn.timeline-service.webapp.https.address" : "%HOSTGROUP::master_2%:8190"\n        }\n      }\n    }\n  ]\n}\n')),(0,r.kt)("h4",{id:"register-blueprint-with-ambari-server"},"Register Blueprint with Ambari Server"),(0,r.kt)("p",null,'Post the blueprint to the "blueprint-yarn-ha" resource to the Ambari Server.'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"POST /api/v1/blueprints/blueprint-yarn-ha\n \n...\n[ Request Body is the example blueprint defined above ]\n...\n \n201 - Created\n\n")),(0,r.kt)("h4",{id:"example-cluster-creation-template-1"},"Example Cluster Creation Template"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "blueprint": "blueprint-yarn-ha",\n  "default_password": "changethis",\n  "configurations": [\n    { "yarn-site" : {\n        "yarn.resourcemanager.zk-address" : "c6402.ambari.apache.org:2181,c6403.ambari.apache.org:2181,c6404.ambari.apache.org:2181\u201d,\n        \u201dyarn.resourcemanager.hostname.rm1" : "c6403.ambari.apache.org",\n        "yarn.resourcemanager.hostname.rm2" : "c6404.ambari.apache.org"\n     }}\n  ],\n  "host_groups": [\n    { "hosts": [\n        { "fqdn": "c6401.ambari.apache.org" }\n      ], "name": "gateway"\n    },\n    { "hosts": [\n        { "fqdn": "c6402.ambari.apache.org" }\n      ], "name": "master_1"\n    },\n    { "hosts": [\n        { "fqdn": "c6403.ambari.apache.org" }\n      ], "name": "master_2"\n    },\n    { "hosts": [\n        { "fqdn": "c6404.ambari.apache.org" }\n      ], "name": "master_3"\n    },\n    { "hosts": [\n        { "fqdn": "c6405.ambari.apache.org" }\n      ],\n      "name": "slave_1"     \n    }\n  ]\n}\n')),(0,r.kt)("h4",{id:"create-cluster-instance-1"},"Create Cluster Instance"),(0,r.kt)("p",null,"Post the cluster to the Ambari Server to provision the cluster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'POST /api/v1/clusters/my-yarn-ha-cluster\n \n...\n[ Request Body is above Cluster Creation Template ]\n...\n \n202 - Accepted\n{\n  "href" : "http://c6401.ambari.apache.org:8080/api/v1/clusters/my-yarn-ha-cluster/requests/1",\n  "Requests" : {\n    "id" : 1,\n    "status" : "InProgress"\n  }\n}\n \n...\n[ Client can then monitor the URL in the 202 response to check the status of the cluster deployment. ]\n...\n')),(0,r.kt)("h3",{id:"blueprint-example-hbase-regionserver-ha-cluster"},"Blueprint Example: HBase RegionServer HA Cluster"),(0,r.kt)("h4",{id:"summary-2"},"Summary"),(0,r.kt)("p",null,"HBase provides a High Availability feature for reads across HBase Region Servers. \xa0"),(0,r.kt)("p",null,"The following link to the Apache HBase documentation provides more information on HA support in HBase:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"http://hbase.apache.org/book.html#arch.timelineconsistent.reads"},"http://hbase.apache.org/book.html#arch.timelineconsistent.reads")),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"The documentation listed here explains how to deploy an HBase RegionServer HA cluster via a Blueprint, but there are separate application-specific steps that must be taken in order to enable this feature for a specific table in HBase. \xa0A table must be created with replication enabled, so that multiple Region Servers can handle the keys for this table.")),(0,r.kt)("p",null,"For more information on how to define an HBase table with replication enabled (after the cluster has been created), please refer to the following HBase documentation:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"http://hbase.apache.org/book.html#_creating_a_table_with_region_replication"},"http://hbase.apache.org/book.html#_creating_a_table_with_region_replication")),(0,r.kt)("p",null,"The following stack components should be included in any host group in a Blueprint that supports an HA HBase RegionServer:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"HBASE_REGIONSERVER")),(0,r.kt)("p",null,"At least two \u201cHBASE_REGIONSERVER\u201d components must be deployed in order to enable this feature, so that table information can be replicated across more than one Region Server. \xa0"),(0,r.kt)("h4",{id:"example-blueprint-2"},"Example Blueprint"),(0,r.kt)("p",null,"The following link includes an example Blueprint for a 2-node HBase RegionServer HA Cluster:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/download/attachments/55151584/hbase_rs_ha_blueprint.json?version=1&modificationDate=1427136904000&api=v2"},"hbase_rs_ha_blueprint.json"),"  "),(0,r.kt)("p",null,"The following JSON snippet includes the \u201chbase-site\u201d configuration typically required for a cluster that utilizes the HBase RegionServer HA feature: \xa0"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "configurations" : [\n    {\n      "hbase-site" : {\n         ... \n        "hbase.regionserver.global.memstore.lowerLimit" : "0.38",\n        "hbase.regionserver.global.memstore.upperLimit" : "0.4",\n        "hbase.regionserver.handler.count" : "60",\n        "hbase.regionserver.info.port" : "60030",\n        "hbase.regionserver.storefile.refresh.period" : "20",\n        "hbase.rootdir" : "hdfs://%HOSTGROUP::host_group_1%:8020/apps/hbase/data",\n        "hbase.security.authentication" : "simple",\n        "hbase.security.authorization" : "false",\n        "hbase.superuser" : "hbase",\n        "hbase.tmp.dir" : "/hadoop/hbase",\n        "hbase.zookeeper.property.clientPort" : "2181",\n        "hbase.zookeeper.quorum" : "%HOSTGROUP::host_group_1%,%HOSTGROUP::host_group_2%",\n        "hbase.zookeeper.useMulti" : "true",\n        "hfile.block.cache.size" : "0.40",\n        "zookeeper.session.timeout" : "30000",\n        "zookeeper.znode.parent" : "/hbase-unsecure"\n      }\n\n    }\n   ]\n}\n')),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"The JSON example above is not a complete set of \u201chbase-site\u201d configurations, but rather shows the configuration settings that are relevant to HBase RegionServer HA. \xa0In particular, the \u201chbase.regionserver.storefile.refresh.period\u201d setting is the most relevant to HBase RegionServer HA, since this property must be set to a value greater than zero in order for the HA feature to be enabled.")),(0,r.kt)("h4",{id:"register-blueprint-with-ambari-server-1"},"Register Blueprint with Ambari Server"),(0,r.kt)("p",null,'Post the blueprint to the "blueprint-hbase-rs-ha" resource to the Ambari Server.'),(0,r.kt)("p",null,"POST /api/v1/blueprints/blueprint-hbase-rs-ha"),(0,r.kt)("p",null,"...\n","[ Request Body is the example blueprint defined above ]","\n..."),(0,r.kt)("p",null,"201 - Created"),(0,r.kt)("h4",{id:"example-cluster-creation-template-2"},"Example Cluster Creation Template"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "blueprint" : "blueprint-hbase-rs-ha",\n  "default_password" : "default",\n  "host_groups" :[\n    {\n      "name" : "host_group_1", \n      "hosts" : [         \n        {\n          "fqdn" : "c6401.ambari.apache.org"\n        }\n      ]\n    },\n    {\n      "name" : "host_group_2", \n      "hosts" : [         \n        {\n          "fqdn" : "c6402.ambari.apache.org"\n        }\n      ]\n    }\n  ]\n}\n')),(0,r.kt)("h4",{id:"create-cluster-instance-2"},"Create Cluster Instance"),(0,r.kt)("p",null,"Post the cluster to the Ambari Server to provision the cluster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'POST /api/v1/clusters/my-hbase-rs-ha-cluster\n \n...\n[ Request Body is above Cluster Creation Template ]\n...\n \n202 - Accepted\n{\n  "href" : "http://c6401.ambari.apache.org:8080/api/v1/clusters/my-hbase-rs-ha-cluster/requests/1",\n  "Requests" : {\n    "id" : 1,\n    "status" : "InProgress"\n  }\n}\n \n...\n[ Client can then monitor the URL in the 202 response to check the status of the cluster deployment. ]\n...\n')))}h.isMDXComponent=!0}}]);